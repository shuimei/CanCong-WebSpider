# 智能爬虫调度脚本使用说明

## 🎯 功能概述

智能爬虫调度脚本（`spider_scheduler.py`）是一个自动化工具，支持多进程并发抓取，用于循环随机选择数据库中的未抓取URL，启动爬虫任务，直到完成所有URL的抓取。

### 新增特性：
- 🚀 **多进程并发**：支持同时运行多个爬虫进程
- ⚙️ **可配置并发数**：灵活设置1-10个worker
- 📊 **实时监控**：显示活跃worker数量和当前任务
- 🔒 **线程安全**：使用锁机制保证数据一致性

## 📁 相关文件

- `scripts/spider_scheduler.py` - 主调度脚本
- `spider_urls.db` - SQLite数据库（存储URL状态）
- `spider.py` - 被调度的爬虫主脚本

## 🚀 使用方法

### 1. 基本使用

```bash
# 启动智能调度器
python scripts\spider_scheduler.py
```

### 2. 查看数据库统计

```bash
# 只显示统计信息，不启动调度
python scripts\spider_scheduler.py --stats
```

### 3. 多进程并发配置

```bash
# 设置2个并发worker（默认）
python scripts\spider_scheduler.py --workers 2

# 设置4个并发worker，适合处理大量任务
python scripts\spider_scheduler.py --workers 4

# 单进程模式（兼容老版本）
python scripts\spider_scheduler.py --workers 1

# 高并发模式（最多10个）
python scripts\spider_scheduler.py --workers 8
```

### 4. 组合参数配置

```bash
# 自定义任务间延迟（默认10秒）
python scripts\spider_scheduler.py --delay 5

# 指定项目目录
python scripts\spider_scheduler.py --project-dir /path/to/project

# 组合参数：4个worker + 5秒延迟
python scripts\spider_scheduler.py --workers 4 --delay 5

# 查看特定配置的统计信息
python scripts\spider_scheduler.py --workers 6 --stats
```

## ⚙️ 工作流程

### 1. 初始化阶段
- 检查数据库文件是否存在
- 清理异常状态（长时间处于crawling状态的URL）
- 获取初始统计信息

### 2. 循环调度阶段
```
开始 → 检查待抓取URL → 创建Worker池 → 随机分配URL → 并发执行 → 等待完成 → 更新统计 → 继续循环
```

### 3. 多进程任务执行
- 使用ThreadPoolExecutor管理worker池
- 每个worker独立标记URL为"crawling"状态
- 并发启动多个子进程运行爬虫
- 实时显示每个worker的输出日志
- 根据返回码判断成功/失败
- 线程安全的统计更新

### 4. 退出处理
- 优雅处理中断信号（Ctrl+C）
- 自动终止所有活跃爬虫进程
- 等待worker池完全清理
- 显示最终统计信息

## 📊 状态管理

### URL状态类型
- `pending` - 待抓取
- `crawling` - 正在抓取
- `completed` - 已完成
- `failed` - 抓取失败

### 异常状态清理
自动将超过1小时仍为`crawling`状态的URL重置为`pending`，防止异常退出导致的状态错误。

## 📈 监控功能

### 实时统计显示
```
📊 当前进度:
  总URL数: 150
  已完成: 45 (30.0%)
  待抓取: 98
  抓取中: 4
  失败: 6
  当前活跃Worker: 4
  本次任务数: 12
  活跃任务:
    Worker-1: https://example1.com/page1...
    Worker-2: https://example2.com/page2...
    Worker-3: https://example3.com/page3...
    Worker-4: https://example4.com/page4...
  运行时间: 25.3分钟
  平均速度: 28.4任务/小时
```

### 任务日志
- 显示当前抓取的URL
- 实时输出爬虫执行日志
- 记录任务开始/完成时间
- 显示返回码和状态

## 🛡️ 安全特性

### 信号处理
- `SIGINT` (Ctrl+C) - 优雅退出
- `SIGTERM` - 终止信号处理
- 自动终止子进程避免僵尸进程

### 进程管理
- 实时监控子进程状态
- 超时强制终止机制
- 资源清理和状态同步

### 错误恢复
- 子进程异常自动恢复
- 数据库连接异常处理
- 文件系统错误容错

## 🎨 使用示例

### 场景1：首次批量抓取
```bash
# 1. 先添加一些URL到数据库
python spider.py https://mnr.gov.cn --depth 1

# 2. 启动调度器完成所有任务
python scripts\spider_scheduler.py
```

### 场景2：检查进度
```bash
# 查看当前统计
python scripts\spider_scheduler.py --stats

# 输出示例：
# 📊 数据库统计信息:
#   总URL数: 500
#   已完成: 320
#   待抓取: 150
#   抓取中: 0
#   失败: 30
```

### 场景3：快速处理
```bash
# 减少任务间延迟，加快处理速度
python scripts\spider_scheduler.py --delay 3
```

### 场景5：高效并发处理
```bash
# 使用多个worker并发处理大量任务
python scripts\spider_scheduler.py --workers 4 --delay 3

# 查看并发效果
python scripts\spider_scheduler.py --workers 4 --stats
```

### 场景6：大型项目
```bash
# 适中的并发数和延迟，避免对目标服务器造成压力
python scripts\spider_scheduler.py --workers 2 --delay 30
```

## ⚠️ 注意事项

### 系统要求
- Python 3.7+
- 已安装项目依赖
- 有效的spider.py主脚本
- SQLite数据库支持

### 使用建议
1. **合理设置并发数**：根据目标服务器性能调整worker数量
2. **合理设置延迟**：避免对目标服务器造成过大压力
3. **监控资源使用**：大量任务时注意磁盘空间和网络带宽
4. **定期检查日志**：及时发现和处理异常情况
5. **备份数据库**：重要数据定期备份

### 限制说明
- 支持多进程并发（max_concurrent=1-10）
- 依赖spider.py脚本的正确配置
- 需要数据库中已有URL记录才能工作
- 线程池大小受系统资源限制

## 🔧 自定义配置

### 修改默认参数
编辑脚本中的默认值：
```python
def __init__(self, project_dir=None, max_concurrent=2, delay_between_tasks=10):
    self.max_concurrent = max_concurrent  # 修改默认并发数
    self.delay_between_tasks = delay_between_tasks  # 修改默认延迟
```

### 修改爬虫命令
编辑`run_spider_for_url`方法中的命令：
```python
cmd = [
    sys.executable,
    str(spider_script),
    url,
    '--depth', '3',  # 修改默认深度
    '--delay', '2'   # 添加额外参数
]
```

### 修改状态清理策略
编辑`clean_stale_crawling_status`方法：
```python
# 将超时时间从1小时改为30分钟
AND datetime(last_crawl_time) < datetime('now', '-30 minutes')
```

## 🎉 优势特点

### 1. 智能随机调度
- 真正的随机选择算法
- 多进程并发提高效率
- 避免按顺序抓取的可预测性
- 提高抓取效率和成功率

### 2. 完善的状态管理
- 实时同步数据库状态
- 自动处理异常状态
- 支持中断后继续

### 3. 友好的用户界面
- 彩色状态提示
- 实时进度显示
- 详细的统计信息

### 4. 健壮的错误处理
- 优雅退出机制
- 多进程管理和清理
- 线程安全的资源管理
- 异常恢复能力

这个调度脚本特别适合需要批量处理大量URL的场景，支持多进程并发抓取，可以显著提高爬虫工作的自动化程度和可靠性。