# 🕷️ 网页爬虫系统 - 完整使用指南

## 📖 系统概述

这是一个基于Scrapy和FastAPI的智能网页爬虫系统，具备以下特性：

- **智能抓取**: 支持JavaScript渲染，自动过滤静态资源
- **Web监控界面**: 实时查看抓取进度和结果
- **去重机制**: SQLite数据库记录URL状态，避免重复抓取
- **内容存储**: 自动保存HTML文件到本地
- **错误处理**: 完善的错误处理和重试机制

## 🚀 快速开始

### 1. 安装依赖

```bash
# 方式1：使用安装脚本（Windows推荐）
install.bat

# 方式2：手动安装
pip install -r requirements.txt
```

### 2. 启动Web监控界面（强烈推荐）

```bash
# 方式1：Python脚本
python start_monitor.py

# 方式2：批处理文件（Windows）
start_monitor.bat

# 方式3：直接启动FastAPI
cd frontend
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

启动后访问：
- **主界面**: http://localhost:8000
- **API文档**: http://localhost:8000/docs

### 3. 运行爬虫

```bash
# 基本用法
python spider.py https://example.com

# 随机抓取（从数据库中选择待抓取URL）
python spider.py --random
python spider.py -r --depth 3

# 指定抓取深度和延迟
python spider.py https://example.com --depth 3 --delay 1

# 高并发抓取
python spider.py https://example.com --concurrent 8 --delay 0.5

# 交互式模式（会自动检测并提示随机模式）
python spider.py
```

## 🖥️ Web监控界面功能

### 实时监控面板
- **统计卡片**: 显示总URL数、成功数、失败数、待抓取数
- **状态图表**: 饼图显示抓取状态分布
- **实时更新**: WebSocket自动推送最新数据

### 页面管理
- **页面列表**: 浏览所有已抓取的网页
- **在线预览**: 直接在浏览器中查看HTML页面
- **下载功能**: 下载保存的HTML文件
- **搜索过滤**: 按URL或标题搜索页面

### 错误分析
- **失败页面**: 查看抓取失败的URL列表
- **错误原因**: 详细的失败原因说明
- **统计分析**: 成功率和失败模式分析

## 🛠️ 命令行参数

| 参数 | 简写 | 默认值 | 说明 |
|------|------|--------|------|
| `--depth` | `-d` | 2 | 最大抓取深度 |
| `--delay` | | 2 | 请求间隔秒数 |
| `--concurrent` | `-c` | 4 | 并发请求数量 |
| `--output` | `-o` | webpages | HTML文件输出目录 |
| `--database` | `--db` | spider_urls.db | SQLite数据库文件 |
| `--stats` | | | 显示统计信息后退出 |
| `--clean` | | | 清理长时间未完成的任务 |
| `--reset` | | | 重置数据库（删除所有数据） |

## 📊 使用示例

### 基础抓取
```bash
# 抓取一个网站，深度2层
python spider.py https://news.example.com

# 抓取到指定目录
python spider.py https://docs.example.com --output ./documentation
```

### 随机抓取模式
```bash
# 从数据库中随机选择一个待抓取的URL
python spider.py --random --depth 2

# 随机抓取，结合其他参数
python spider.py -r -d 3 --delay 1 --concurrent 8

# 如果数据库中没有待抓取URL，可以提供备用URL
python spider.py --random https://backup-site.com
```

### 高级配置
```bash
# 深度抓取，低延迟，高并发
python spider.py https://blog.example.com --depth 4 --delay 0.5 --concurrent 16

# 快速浅层抓取
python spider.py https://api-docs.example.com --depth 1 --delay 0.2 --concurrent 20
```

### 管理维护
```bash
# 查看抓取统计
python spider.py --stats

# 清理卡住的任务
python spider.py --clean

# 重新开始（删除所有数据）
python spider.py --reset
```

## 🔧 技术架构

### 核心组件
- **Scrapy框架**: 高性能的网页抓取引擎
- **Selenium**: JavaScript渲染支持
- **SQLite数据库**: URL状态管理和去重
- **FastAPI**: 现代化的API服务器
- **Vue.js**: 响应式前端界面

### 数据流程
1. **URL发现**: 从页面中提取链接
2. **去重检查**: 查询数据库避免重复抓取
3. **内容过滤**: 智能过滤静态资源
4. **JavaScript渲染**: 对需要的页面执行JS
5. **内容保存**: 存储HTML文件和元数据
6. **状态更新**: 实时更新抓取状态

## 📁 文件结构

```
spider/
├── spider.py                 # 主启动脚本
├── start_monitor.py          # Web监控启动脚本
├── start_monitor.bat         # Windows批处理
├── run_spider.py             # 爬虫核心逻辑
├── test_spider.py           # 功能测试
├── requirements.txt          # Python依赖
├── README.md                # 项目文档
├── 修复说明.md              # 问题修复记录
├── webspider/               # 爬虫核心模块
│   ├── settings.py          # Scrapy配置
│   ├── items.py             # 数据项定义
│   ├── database.py          # 数据库管理
│   ├── middlewares.py       # 中间件
│   ├── pipelines.py         # 数据处理管道
│   └── spiders/
│       └── webspider.py     # 主爬虫（已修复）
├── frontend/                # Web监控界面
│   ├── main.py              # FastAPI服务器
│   └── templates/
│       └── index.html       # 前端页面
├── webpages/                # HTML文件存储
└── spider_urls.db           # SQLite数据库（运行时创建）
```

## ⚠️ 重要更新

### 🐛 已修复问题
- **非文本内容错误**: 修复了处理图片等二进制文件时的崩溃问题
- **URL过滤优化**: 智能过滤静态资源，提高抓取效率
- **错误处理增强**: 完善的异常处理和日志记录

### 🆕 新增功能
- **Web监控界面**: 基于FastAPI的现代化监控台
- **实时通信**: WebSocket实时推送抓取状态
- **智能过滤**: 自动识别和跳过非HTML内容
- **内容验证**: 验证HTML内容的有效性

## 💡 最佳实践

### 性能优化
1. **合理设置参数**:
   - 深度不宜过大（建议2-4层）
   - 并发数根据目标服务器性能调整
   - 延迟设置要考虑服务器负载

2. **监控和调试**:
   - 使用Web界面实时监控进度
   - 定期检查失败页面的错误原因
   - 根据统计信息调整抓取策略

### 合规使用
1. **遵守robots.txt**: 虽然可以忽略，但建议遵守
2. **尊重服务器**: 合理设置延迟，避免过载
3. **法律合规**: 确保抓取行为符合相关法律法规

## 🚀 进阶功能

### 自定义配置
可以修改 `webspider/settings.py` 自定义Scrapy配置：
```python
# 调整并发和延迟
CONCURRENT_REQUESTS = 8
DOWNLOAD_DELAY = 1

# 启用额外的中间件
DOWNLOADER_MIDDLEWARES = {
    'webspider.middlewares.CustomMiddleware': 585,
}
```

### API集成
Web监控界面提供完整的RESTful API：
```bash
# 获取统计信息
curl http://localhost:8000/api/stats

# 搜索页面
curl "http://localhost:8000/api/search?q=关键词"

# 下载页面
curl -O http://localhost:8000/api/download/filename.html
```

---

🎉 **现在就开始使用您的智能网页爬虫系统吧！**

如有问题，请查看 `修复说明.md` 了解最新的修复记录和使用建议。