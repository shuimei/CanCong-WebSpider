# 🎲 随机抓取功能说明

## 功能概述

新增了随机抓取功能，可以从数据库中随机选择一个待抓取的URL开始新的抓取任务。这个功能特别适合以下场景：

- **继续之前的抓取任务**: 从上次中断的地方随机选择继续
- **负载均衡**: 避免总是从同一个域名开始抓取
- **探索性抓取**: 随机发现新的有趣内容
- **断点续传**: 无需手动记住上次抓取到哪里

## 🚀 使用方法

### 1. 命令行模式

```bash
# 随机选择一个待抓取的URL开始
python spider.py --random

# 或使用简写
python spider.py -r

# 结合其他参数
python spider.py --random --depth 3 --delay 1 --concurrent 8

# 如果数据库中没有待抓取URL，可以提供备用URL
python spider.py --random https://backup-url.com
```

### 2. 交互式模式

运行 `python spider.py` 进入交互式模式时，如果检测到数据库中有待抓取的URL，会提示是否使用随机模式：

```
网页爬虫 - 交互式模式
==============================
数据库中有 156 个待抓取的URL
是否使用随机选择模式? (y/N): y
随机选择URL: https://example.com/page/123
最大抓取深度 (默认2): 3
请求延迟秒数 (默认2): 1

开始随机抓取...
```

## 📊 工作原理

### 数据库查询
使用SQL的 `ORDER BY RANDOM()` 语句从待抓取的URL中随机选择一个：

```sql
SELECT url, source_url, depth FROM urls 
WHERE status = 'pending' 
ORDER BY RANDOM() 
LIMIT 1
```

### 智能回退
- **优先使用随机URL**: 如果数据库中有待抓取的URL，优先使用随机选择的URL
- **备用URL支持**: 如果没有待抓取的URL，可以使用命令行提供的备用URL
- **错误提示**: 如果既没有待抓取URL又没有提供备用URL，会给出清晰的错误提示

## 💡 使用场景

### 场景1：续传大型抓取任务
```bash
# 第一次抓取
python spider.py https://news.example.com --depth 4

# 中断后继续抓取（从随机位置开始）
python spider.py --random --depth 4
```

### 场景2：多轮探索
```bash
# 第一轮：浅层抓取收集URL
python spider.py https://site.com --depth 1

# 第二轮：随机深度抓取
python spider.py --random --depth 3

# 第三轮：再次随机抓取
python spider.py --random --depth 2
```

### 场景3：多域名负载均衡
```bash
# 抓取多个起始站点后
python spider.py site1.com --depth 1
python spider.py site2.com --depth 1
python spider.py site3.com --depth 1

# 随机选择继续抓取
python spider.py --random --depth 3
```

## 🔧 命令行参数

| 参数 | 简写 | 描述 | 示例 |
|------|------|------|------|
| `--random` | `-r` | 随机选择待抓取URL | `python spider.py --random` |
| 备用URL | | 如果没有待抓取URL时使用 | `python spider.py --random https://backup.com` |

## 📈 优势特点

### 1. **智能选择**
- 自动检测数据库中的待抓取URL
- 智能回退到备用URL
- 清晰的错误提示和状态反馈

### 2. **负载分散**
- 避免总是从同一个起点开始
- 减少对特定服务器的压力
- 提高抓取效率

### 3. **用户友好**
- 交互式模式自动提示
- 支持所有现有参数
- 保持与原有功能的兼容性

### 4. **状态透明**
- 显示随机选择的URL
- 明确标识随机模式
- 完整的统计信息

## 🎯 实际效果

### 运行示例

```bash
$ python spider.py --random --depth 2
从数据库中随机选择URL: https://docs.example.com/api/v2/users
开始爬取: https://docs.example.com/api/v2/users
最大深度: 2
输出目录: webpages
数据库文件: spider_urls.db
模式: 随机选择待抓取URL
--------------------------------------------------
```

### Web监控界面显示
在Web监控界面中可以看到：
- 随机选择的URL被标记为"正在抓取"
- 实时统计信息更新
- 新发现的URL被添加到待抓取队列

## ⚠️ 注意事项

### 1. **数据库依赖**
- 需要数据库中有 `status = 'pending'` 的URL
- 首次使用前需要先进行一次普通抓取来收集URL

### 2. **随机性**
- 每次运行都会选择不同的URL（除非只有一个待抓取URL）
- 无法预测具体会选择哪个URL

### 3. **性能考虑**
- 随机选择是瞬时完成的，不会影响性能
- 适合各种规模的URL队列

## 🔍 故障排除

### 问题：提示"数据库中没有待抓取的URL"
**解决方法**：
```bash
# 先进行一次普通抓取收集URL
python spider.py https://example.com --depth 1

# 然后再使用随机模式
python spider.py --random
```

### 问题：总是选择相同的URL
**解决方法**：
- 检查数据库中是否只有一个待抓取的URL
- 使用 `python spider.py --stats` 查看待抓取URL数量

### 问题：随机选择的URL无法访问
**解决方法**：
- 爬虫会自动处理失败的URL
- 失败的URL会被标记为 'failed' 状态
- 下次随机选择时会跳过失败的URL

## 🎉 总结

随机抓取功能为网页爬虫增加了更多的灵活性和智能化，特别适合大规模、长期的抓取任务。结合现有的Web监控界面，可以更好地管理和监控抓取进度。

**主要优势**：
- ✅ 智能续传，无需手动指定URL
- ✅ 负载均衡，避免集中抓取
- ✅ 用户友好，交互式提示
- ✅ 完全兼容现有功能
- ✅ 实时监控和统计

现在您可以更高效地使用爬虫系统，让它自动选择最适合的URL继续抓取任务！