#!/usr/bin/env python3
"""
å¤šURLçˆ¬è™«è¿è¡Œè„šæœ¬
æ”¯æŒå¤šä¸ªèµ·å§‹URLã€å¤šè¿›ç¨‹å¹¶å‘å’ŒJavaScriptæ¸²æŸ“
ä¸“ä¸ºWebç•Œé¢è°ƒç”¨ä¼˜åŒ–
"""

import os
import sys
import argparse
import json
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from webspider.database import UrlDatabase
from webspider.spiders.webspider import WebSpider


class MultiCrawlerRunner:
    """å¤šURLçˆ¬è™«è¿è¡Œå™¨"""
    
    def __init__(self, database_path: str, output_dir: str, workers: int = 2):
        self.database_path = database_path
        self.output_dir = output_dir
        self.workers = workers
        self.db = UrlDatabase(database_path)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output_dir).mkdir(exist_ok=True)
        
        print(f"åˆå§‹åŒ–çˆ¬è™«è¿è¡Œå™¨:")
        print(f"  æ•°æ®åº“è·¯å¾„: {database_path}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  å¹¶å‘æ•°: {workers}")
    
    def run_single_crawler(self, start_url: str, max_depth: int = 3, 
                          enable_js: bool = False, worker_id: int = 0):
        """è¿è¡Œå•ä¸ªçˆ¬è™«å®ä¾‹"""
        try:
            print(f"[Worker-{worker_id}] å¼€å§‹æŠ“å–: {start_url}")
            
            # é…ç½®Scrapyè®¾ç½®
            settings = get_project_settings()
            settings.set('DOWNLOAD_DELAY', 1.5)  # ç¨å¾®å‡å°‘å»¶è¿Ÿä»¥æé«˜æ•ˆç‡
            settings.set('CONCURRENT_REQUESTS', 2)  # æ¯ä¸ªè¿›ç¨‹å†…éƒ¨å¹¶å‘
            settings.set('WEBPAGES_DIR', self.output_dir)
            settings.set('DATABASE_URL', self.database_path)
            
            # è®¾ç½®USER_AGENT
            settings.set('USER_AGENT', f'WebCrawler-Worker-{worker_id} (+http://www.example.com/bot)')
            
            # å¯ç”¨ç®¡é“
            settings.set('ITEM_PIPELINES', {
                'webspider.pipelines.UrlFilterPipeline': 300,
                'webspider.pipelines.HtmlSavePipeline': 400,
                'webspider.pipelines.StatisticsPipeline': 500,
            })
            
            # JavaScriptæ¸²æŸ“é…ç½®
            if enable_js:
                print(f"[Worker-{worker_id}] JavaScriptæ¸²æŸ“å·²å¯ç”¨")
                # è¿™é‡Œå¯ä»¥æ·»åŠ Seleniumç›¸å…³é…ç½®
            
            # å¯åŠ¨çˆ¬è™«
            process = CrawlerProcess(settings)
            process.crawl(
                WebSpider, 
                start_url=start_url, 
                max_depth=max_depth, 
                enable_keyword_filter=True
            )
            process.start()
            
            print(f"[Worker-{worker_id}] å®ŒæˆæŠ“å–: {start_url}")
            return True
            
        except Exception as e:
            print(f"[Worker-{worker_id}] æŠ“å–å¤±è´¥ {start_url}: {e}")
            return False
    
    def run_multi_crawlers(self, start_urls: list, max_depth: int = 3, enable_js: bool = False):
        """è¿è¡Œå¤šä¸ªçˆ¬è™«å®ä¾‹"""
        if not start_urls:
            print("é”™è¯¯: æ²¡æœ‰æä¾›èµ·å§‹URL")
            return False
        
        print(f"å¼€å§‹å¤šè¿›ç¨‹æŠ“å–ï¼Œå…± {len(start_urls)} ä¸ªURLï¼Œ{self.workers} ä¸ªworker")
        print("-" * 60)
        
        # è®°å½•èµ·å§‹æ—¶é—´
        start_time = time.time()
        
        # æ˜¾ç¤ºæŠ“å–è®¡åˆ’
        for i, url in enumerate(start_urls):
            print(f"  {i+1}. {url}")
        print("-" * 60)
        
        successful_count = 0
        failed_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¤šä¸ªçˆ¬è™«
        with ThreadPoolExecutor(max_workers=self.workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_url = {}
            for i, url in enumerate(start_urls):
                future = executor.submit(
                    self.run_single_crawler, 
                    url, 
                    max_depth, 
                    enable_js, 
                    i + 1
                )
                future_to_url[future] = url
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    success = future.result()
                    if success:
                        successful_count += 1
                        print(f"âœ“ æˆåŠŸå®Œæˆ: {url}")
                    else:
                        failed_count += 1
                        print(f"âœ— æŠ“å–å¤±è´¥: {url}")
                except Exception as e:
                    failed_count += 1
                    print(f"âœ— æ‰§è¡Œå¼‚å¸¸: {url} - {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print("\n" + "=" * 60)
        print("æŠ“å–ä»»åŠ¡å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»URLæ•°:     {len(start_urls)}")
        print(f"æˆåŠŸå®Œæˆ:    {successful_count}")
        print(f"å¤±è´¥:        {failed_count}")
        print(f"æ€»è€—æ—¶:      {elapsed_time:.1f} ç§’")
        
        if len(start_urls) > 0:
            success_rate = (successful_count / len(start_urls)) * 100
            print(f"æˆåŠŸç‡:      {success_rate:.1f}%")
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
        print("\næ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        self.show_database_stats()
        
        return successful_count > 0
    
    def show_database_stats(self):
        """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.db.get_stats()
            print(f"  æ•°æ®åº“æ€»URL: {stats['total']}")
            print(f"  å¾…æŠ“å–:      {stats['pending']}")
            print(f"  æˆåŠŸ:        {stats['success']}")
            print(f"  å¤±è´¥:        {stats['failed']}")
            print(f"  è¿›è¡Œä¸­:      {stats['crawling']}")
        except Exception as e:
            print(f"  è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šURLç½‘é¡µçˆ¬è™«ç¨‹åº')
    parser.add_argument('--url', action='append', help='èµ·å§‹URLåœ°å€ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰')
    parser.add_argument('--urls-file', help='åŒ…å«URLåˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--depth', '-d', type=int, default=3, help='æœ€å¤§æŠ“å–æ·±åº¦ (é»˜è®¤: 3)')
    parser.add_argument('--workers', '-w', type=int, default=2, 
                       help='å¹¶å‘workeræ•° (é»˜è®¤: 2, èŒƒå›´: 1-10)')
    parser.add_argument('--output', '-o', default='webpages', help='è¾“å‡ºç›®å½• (é»˜è®¤: webpages)')
    parser.add_argument('--database', '--db', default='spider_urls.db', 
                       help='æ•°æ®åº“æ–‡ä»¶ (é»˜è®¤: spider_urls.db)')
    parser.add_argument('--enable-js', action='store_true', help='å¯ç”¨JavaScriptæ¸²æŸ“')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯åé€€å‡º')
    parser.add_argument('--clean', action='store_true', help='æ¸…ç†æ•°æ®åº“ä¸­é•¿æ—¶é—´æœªå®Œæˆçš„ä»»åŠ¡')
    parser.add_argument('--random', '-r', action='store_true', help='ä»æ•°æ®åº“ä¸­éšæœºé€‰æ‹©å¾…æŠ“å–çš„URLå¼€å§‹')
    
    args = parser.parse_args()
    
    # éªŒè¯workerså‚æ•°
    if args.workers < 1 or args.workers > 10:
        print(f"é”™è¯¯: workersæ•°é‡å¿…é¡»åœ¨1-10ä¹‹é—´ï¼Œå½“å‰å€¼: {args.workers}")
        return 1
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = UrlDatabase(args.database)
    
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if args.stats:
        show_database_stats(db)
        return 0
    
    if args.clean:
        count = db.cleanup_stale_crawling()
        print(f"å·²æ¸…ç† {count} ä¸ªé•¿æ—¶é—´æœªå®Œæˆçš„ä»»åŠ¡")
        show_database_stats(db)
        return 0
    
    # è·å–URLåˆ—è¡¨
    start_urls = []
    
    # éšæœºé€‰æ‹©URLåŠŸèƒ½
    if args.random:
        print("ğŸ² ä½¿ç”¨éšæœºæ¨¡å¼é€‰æ‹©å¾…æŠ“å–URL")
        
        # ç¡®å®šéœ€è¦è·å–çš„éšæœº URL æ•°é‡
        random_count = args.workers  # é»˜è®¤ä¸º worker æ•°é‡
        
        # ä»æ•°æ®åº“éšæœºè·å–å¤šä¸ª URL
        random_urls = []
        for i in range(random_count):
            random_url_info = db.get_random_pending_url()
            if random_url_info:
                random_urls.append(random_url_info[0])  # åªéœ€è¦URLå­—ç¬¦ä¸²
        
        if random_urls:
            start_urls.extend(random_urls)
            print(f"ğŸ“Š ä»æ•°æ®åº“éšæœºé€‰æ‹©äº† {len(random_urls)} ä¸ª URL:")
            for i, url in enumerate(random_urls, 1):
                print(f"  {i}. {url}")
        else:
            print("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰å¾…æŠ“å–çš„URL")
            
            # å¦‚æœæä¾›äº†å¤‡ç”¨URLï¼Œä½¿ç”¨å®ƒä»¬
            if args.url:
                print("ğŸ”„ ä½¿ç”¨æä¾›çš„å¤‡ç”¨URL")
                start_urls.extend(args.url)
            
            if args.urls_file:
                print("ğŸ”„ ä½¿ç”¨æä¾›çš„URLæ–‡ä»¶")
                try:
                    with open(args.urls_file, 'r', encoding='utf-8') as f:
                        file_urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                        start_urls.extend(file_urls)
                except FileNotFoundError:
                    print(f"é”™è¯¯: æ‰¾ä¸åˆ°URLæ–‡ä»¶: {args.urls_file}")
                    return 1
                except Exception as e:
                    print(f"é”™è¯¯: è¯»å–URLæ–‡ä»¶å¤±è´¥: {e}")
                    return 1
            
            if not start_urls:
                print("é”™è¯¯: æ•°æ®åº“ä¸­æ²¡æœ‰å¾…æŠ“å–URLï¼Œä¸”æœªæä¾›å¤‡ç”¨URL")
                print("è¯·å…ˆä½¿ç”¨æ™®é€šæ¨¡å¼æ·»åŠ ä¸€äº›URLï¼Œæˆ–è€…æä¾›ä¸€ä¸ªèµ·å§‹ URL")
                return 1
        
        print()
    
    # ééšæœºæ¨¡å¼ï¼šä½¿ç”¨ä¼ ç»ŸURLè·å–æ–¹å¼
    if not args.random:
        if args.url:
            start_urls.extend(args.url)
        
        if args.urls_file:
            try:
                with open(args.urls_file, 'r', encoding='utf-8') as f:
                    file_urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    start_urls.extend(file_urls)
            except FileNotFoundError:
                print(f"é”™è¯¯: æ‰¾ä¸åˆ°URLæ–‡ä»¶: {args.urls_file}")
                return 1
            except Exception as e:
                print(f"é”™è¯¯: è¯»å–URLæ–‡ä»¶å¤±è´¥: {e}")
                return 1
    
    if not start_urls:
        print("é”™è¯¯: å¿…é¡»æä¾›è‡³å°‘ä¸€ä¸ªèµ·å§‹URL")
        print("ä½¿ç”¨ --url URL æˆ– --urls-file FILE æ¥æŒ‡å®šURL")
        parser.print_help()
        return 1
    
    # å»é‡å¹¶éªŒè¯URL
    unique_urls = []
    for url in start_urls:
        if url not in unique_urls:
            if url.startswith('http://') or url.startswith('https://'):
                unique_urls.append(url)
            else:
                print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆURL: {url}")
    
    if not unique_urls:
        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„URL")
        return 1
    
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  URLæ•°é‡: {len(unique_urls)}")
    print(f"  æœ€å¤§æ·±åº¦: {args.depth}")
    print(f"  Workeræ•°: {args.workers}")
    print(f"  è¾“å‡ºç›®å½•: {args.output}")
    print(f"  JavaScriptæ¸²æŸ“: {args.enable_js}")
    if args.random:
        print(f"  æ¨¡å¼: éšæœºé€‰æ‹©å¾…æŠ“å–URL")
    print("")
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    try:
        runner = MultiCrawlerRunner(args.database, args.output, args.workers)
        success = runner.run_multi_crawlers(unique_urls, args.depth, args.enable_js)
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢çˆ¬è™«...")
        return 1
    except Exception as e:
        print(f"çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        return 1


def show_database_stats(db):
    """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    stats = db.get_stats()
    
    print("=" * 40)
    print("æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 40)
    print(f"æ€»URLæ•°:    {stats['total']}")
    print(f"å¾…æŠ“å–:     {stats['pending']}")
    print(f"æŠ“å–æˆåŠŸ:   {stats['success']}")
    print(f"æŠ“å–å¤±è´¥:   {stats['failed']}")
    print(f"æ­£åœ¨æŠ“å–:   {stats['crawling']}")
    print("=" * 40)


if __name__ == "__main__":
    sys.exit(main())