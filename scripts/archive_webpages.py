#!/usr/bin/env python3
"""
ç½‘é¡µæ–‡ä»¶æ‰“åŒ…å½’æ¡£è„šæœ¬
å°†webpagesç›®å½•ä¸­çš„HTMLæ–‡ä»¶æ‰“åŒ…ä¸ºZIPæ–‡ä»¶ï¼Œå¹¶å¯é€‰æ‹©åˆ é™¤åŸæ–‡ä»¶
"""

import os
import zipfile
import shutil
from pathlib import Path
from datetime import datetime
import argparse


class WebpageArchiver:
    """ç½‘é¡µæ–‡ä»¶å½’æ¡£å™¨"""
    
    def __init__(self, webpages_dir="webpages", output_dir="archives"):
        """
        åˆå§‹åŒ–å½’æ¡£å™¨
        
        Args:
            webpages_dir: ç½‘é¡µæ–‡ä»¶ç›®å½•
            output_dir: å½’æ¡£è¾“å‡ºç›®å½•
        """
        self.project_root = Path(__file__).parent.parent
        self.webpages_dir = self.project_root / webpages_dir
        self.output_dir = self.project_root / output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ç½‘é¡µæ–‡ä»¶å½’æ¡£å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æºç›®å½•: {self.webpages_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def get_html_files(self):
        """è·å–æ‰€æœ‰HTMLæ–‡ä»¶åˆ—è¡¨"""
        if not self.webpages_dir.exists():
            print(f"è­¦å‘Š: æºç›®å½•ä¸å­˜åœ¨: {self.webpages_dir}")
            return []
        
        html_files = list(self.webpages_dir.glob('*.html'))
        print(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        return html_files
    
    def create_archive_filename(self):
        """ç”Ÿæˆå½’æ¡£æ–‡ä»¶å"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f"webpages_archive_{timestamp}.zip"
    
    def create_zip_archive(self, html_files, archive_path, compression_level=6):
        """
        åˆ›å»ºZIPå½’æ¡£æ–‡ä»¶
        
        Args:
            html_files: HTMLæ–‡ä»¶åˆ—è¡¨
            archive_path: å½’æ¡£æ–‡ä»¶è·¯å¾„
            compression_level: å‹ç¼©çº§åˆ« (0-9)
        
        Returns:
            æˆåŠŸåˆ›å»ºçš„æ–‡ä»¶æ•°é‡
        """
        success_count = 0
        total_size = 0
        
        try:
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=compression_level) as zipf:
                for html_file in html_files:
                    try:
                        # è·å–æ–‡ä»¶ä¿¡æ¯
                        file_size = html_file.stat().st_size
                        total_size += file_size
                        
                        # æ·»åŠ åˆ°ZIPæ–‡ä»¶ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
                        arcname = html_file.name
                        zipf.write(html_file, arcname)
                        success_count += 1
                        
                        if success_count % 100 == 0:
                            print(f"å·²å¤„ç† {success_count} ä¸ªæ–‡ä»¶...")
                            
                    except Exception as e:
                        print(f"æ·»åŠ æ–‡ä»¶å¤±è´¥ {html_file.name}: {e}")
                        continue
            
            # è·å–å‹ç¼©åæ–‡ä»¶å¤§å°
            compressed_size = archive_path.stat().st_size
            compression_ratio = (1 - compressed_size / total_size) * 100 if total_size > 0 else 0
            
            print(f"\nğŸ“¦ å½’æ¡£åˆ›å»ºå®Œæˆ:")
            print(f"  å½’æ¡£æ–‡ä»¶: {archive_path}")
            print(f"  åŒ…å«æ–‡ä»¶: {success_count} ä¸ª")
            print(f"  åŸå§‹å¤§å°: {self.format_size(total_size)}")
            print(f"  å‹ç¼©å¤§å°: {self.format_size(compressed_size)}")
            print(f"  å‹ç¼©ç‡: {compression_ratio:.1f}%")
            
            return success_count
            
        except Exception as e:
            print(f"åˆ›å»ºå½’æ¡£æ–‡ä»¶å¤±è´¥: {e}")
            return 0
    
    def format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    def delete_original_files(self, html_files, confirmed=False):
        """
        åˆ é™¤åŸå§‹HTMLæ–‡ä»¶
        
        Args:
            html_files: è¦åˆ é™¤çš„HTMLæ–‡ä»¶åˆ—è¡¨
            confirmed: æ˜¯å¦å·²ç¡®è®¤åˆ é™¤
        
        Returns:
            æˆåŠŸåˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        if not confirmed:
            response = input(f"\nâš ï¸  ç¡®è®¤åˆ é™¤ {len(html_files)} ä¸ªåŸå§‹HTMLæ–‡ä»¶? (yes/no): ")
            if response.lower() not in ['yes', 'y', 'æ˜¯']:
                print("å–æ¶ˆåˆ é™¤æ“ä½œ")
                return 0
        
        deleted_count = 0
        
        for html_file in html_files:
            try:
                html_file.unlink()
                deleted_count += 1
                
                if deleted_count % 100 == 0:
                    print(f"å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶...")
                    
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {html_file.name}: {e}")
                continue
        
        print(f"\nğŸ—‘ï¸  åˆ é™¤å®Œæˆ: {deleted_count} ä¸ªæ–‡ä»¶")
        return deleted_count
    
    def archive_and_cleanup(self, delete_originals=False, auto_confirm=False, compression_level=6):
        """
        æ‰§è¡Œå½’æ¡£å’Œæ¸…ç†æ“ä½œ
        
        Args:
            delete_originals: æ˜¯å¦åˆ é™¤åŸæ–‡ä»¶
            auto_confirm: æ˜¯å¦è‡ªåŠ¨ç¡®è®¤åˆ é™¤
            compression_level: å‹ç¼©çº§åˆ«
        
        Returns:
            æ“ä½œç»“æœå­—å…¸
        """
        # è·å–HTMLæ–‡ä»¶åˆ—è¡¨
        html_files = self.get_html_files()
        
        if not html_files:
            print("æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶ï¼Œæ“ä½œå–æ¶ˆ")
            return {'success': False, 'message': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦å½’æ¡£'}
        
        # åˆ›å»ºå½’æ¡£æ–‡ä»¶å
        archive_filename = self.create_archive_filename()
        archive_path = self.output_dir / archive_filename
        
        print(f"\nå¼€å§‹åˆ›å»ºå½’æ¡£æ–‡ä»¶: {archive_filename}")
        
        # åˆ›å»ºZIPå½’æ¡£
        success_count = self.create_zip_archive(html_files, archive_path, compression_level)
        
        if success_count == 0:
            print("å½’æ¡£åˆ›å»ºå¤±è´¥")
            return {'success': False, 'message': 'å½’æ¡£åˆ›å»ºå¤±è´¥'}
        
        # éªŒè¯å½’æ¡£æ–‡ä»¶
        if not self.verify_archive(archive_path, len(html_files)):
            print("âš ï¸  å½’æ¡£æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§")
        
        result = {
            'success': True,
            'archive_path': str(archive_path),
            'archived_files': success_count,
            'deleted_files': 0
        }
        
        # å¯é€‰åˆ é™¤åŸæ–‡ä»¶
        if delete_originals:
            deleted_count = self.delete_original_files(html_files, auto_confirm)
            result['deleted_files'] = deleted_count
            
            if deleted_count == success_count:
                print(f"\nâœ… å½’æ¡£å’Œæ¸…ç†å®Œæˆ!")
            else:
                print(f"\nâš ï¸  å½’æ¡£å®Œæˆï¼Œä½†åˆ é™¤äº† {deleted_count}/{success_count} ä¸ªæ–‡ä»¶")
        else:
            print(f"\nâœ… å½’æ¡£å®Œæˆ! åŸæ–‡ä»¶ä¿ç•™åœ¨ {self.webpages_dir}")
        
        return result
    
    def verify_archive(self, archive_path, expected_count):
        """éªŒè¯å½’æ¡£æ–‡ä»¶å®Œæ•´æ€§"""
        try:
            with zipfile.ZipFile(archive_path, 'r') as zipf:
                # æ£€æŸ¥æ–‡ä»¶æ•°é‡
                actual_count = len(zipf.namelist())
                if actual_count != expected_count:
                    print(f"æ–‡ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected_count}, å®é™… {actual_count}")
                    return False
                
                # æµ‹è¯•æ‰€æœ‰æ–‡ä»¶
                bad_files = zipf.testzip()
                if bad_files:
                    print(f"å‘ç°æŸåçš„æ–‡ä»¶: {bad_files}")
                    return False
                
                print(f"âœ… å½’æ¡£æ–‡ä»¶éªŒè¯æˆåŠŸ: {actual_count} ä¸ªæ–‡ä»¶")
                return True
                
        except Exception as e:
            print(f"éªŒè¯å½’æ¡£æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
    
    def list_archives(self):
        """åˆ—å‡ºå·²æœ‰çš„å½’æ¡£æ–‡ä»¶"""
        archive_files = list(self.output_dir.glob('webpages_archive_*.zip'))
        
        if not archive_files:
            print("æ²¡æœ‰æ‰¾åˆ°å½’æ¡£æ–‡ä»¶")
            return
        
        print(f"\nğŸ“ å·²æœ‰å½’æ¡£æ–‡ä»¶ ({len(archive_files)} ä¸ª):")
        
        # æŒ‰æ—¶é—´æ’åº
        archive_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        for archive_file in archive_files:
            stat = archive_file.stat()
            size = self.format_size(stat.st_size)
            mtime = datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            
            # è·å–ZIPæ–‡ä»¶ä¸­çš„æ–‡ä»¶æ•°é‡
            try:
                with zipfile.ZipFile(archive_file, 'r') as zipf:
                    file_count = len(zipf.namelist())
                print(f"  {archive_file.name} - {size} - {file_count} ä¸ªæ–‡ä»¶ - {mtime}")
            except:
                print(f"  {archive_file.name} - {size} - æœªçŸ¥æ–‡ä»¶æ•° - {mtime}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç½‘é¡µæ–‡ä»¶å½’æ¡£å·¥å…·')
    parser.add_argument('--delete', '-d', action='store_true',
                       help='å½’æ¡£ååˆ é™¤åŸå§‹æ–‡ä»¶')
    parser.add_argument('--yes', '-y', action='store_true',
                       help='è‡ªåŠ¨ç¡®è®¤åˆ é™¤æ“ä½œï¼Œä¸æç¤ºç”¨æˆ·')
    parser.add_argument('--compression', '-c', type=int, default=6, choices=range(0, 10),
                       help='å‹ç¼©çº§åˆ« (0-9, é»˜è®¤6)')
    parser.add_argument('--list', '-l', action='store_true',
                       help='åˆ—å‡ºå·²æœ‰çš„å½’æ¡£æ–‡ä»¶')
    parser.add_argument('--webpages-dir', type=str, default='webpages',
                       help='ç½‘é¡µæ–‡ä»¶ç›®å½• (é»˜è®¤: webpages)')
    parser.add_argument('--output-dir', type=str, default='archives',
                       help='å½’æ¡£è¾“å‡ºç›®å½• (é»˜è®¤: archives)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå½’æ¡£å™¨
    archiver = WebpageArchiver(
        webpages_dir=args.webpages_dir,
        output_dir=args.output_dir
    )
    
    if args.list:
        # åˆ—å‡ºå·²æœ‰å½’æ¡£æ–‡ä»¶
        archiver.list_archives()
        return
    
    # æ‰§è¡Œå½’æ¡£æ“ä½œ
    print(f"\nğŸ—„ï¸  ç½‘é¡µæ–‡ä»¶å½’æ¡£å·¥å…·")
    print("=" * 50)
    
    result = archiver.archive_and_cleanup(
        delete_originals=args.delete,
        auto_confirm=args.yes,
        compression_level=args.compression
    )
    
    if result['success']:
        print(f"\nğŸ“Š æ“ä½œç»Ÿè®¡:")
        print(f"  å½’æ¡£æ–‡ä»¶: {Path(result['archive_path']).name}")
        print(f"  å½’æ¡£æ–‡ä»¶æ•°: {result['archived_files']}")
        if result['deleted_files'] > 0:
            print(f"  åˆ é™¤æ–‡ä»¶æ•°: {result['deleted_files']}")
        
        print(f"\nğŸ’¡ æç¤º:")
        print(f"  - å½’æ¡£æ–‡ä»¶ä¿å­˜åœ¨: {archiver.output_dir}")
        print(f"  - ä½¿ç”¨ --list æŸ¥çœ‹æ‰€æœ‰å½’æ¡£æ–‡ä»¶")
        if not args.delete:
            print(f"  - ä½¿ç”¨ --delete å‚æ•°å¯åœ¨å½’æ¡£ååˆ é™¤åŸæ–‡ä»¶")
    else:
        print(f"\nâŒ æ“ä½œå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")


if __name__ == '__main__':
    main()