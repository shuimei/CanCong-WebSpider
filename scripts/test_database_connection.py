#!/usr/bin/env python3
"""
æµ‹è¯•æ•°æ®åº“è¿æ¥
éªŒè¯è¿æ¥æ± æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
from pathlib import Path
import threading
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from webspider.database import UrlDatabase


def test_single_connection():
    """æµ‹è¯•å•ä¸ªæ•°æ®åº“è¿æ¥"""
    print("æµ‹è¯•å•ä¸ªæ•°æ®åº“è¿æ¥...")
    
    try:
        db = UrlDatabase()
        stats = db.get_stats()
        print("æ•°æ®åº“è¿æ¥æˆåŠŸ!")
        print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
        return True
    except Exception as e:
        print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False


def test_concurrent_connections():
    """æµ‹è¯•å¹¶å‘æ•°æ®åº“è¿æ¥"""
    print("\næµ‹è¯•å¹¶å‘æ•°æ®åº“è¿æ¥...")
    
    results = []
    
    def worker(worker_id):
        try:
            db = UrlDatabase()
            stats = db.get_stats()
            results.append((worker_id, True, f"Worker {worker_id}: æˆåŠŸè·å–ç»Ÿè®¡ä¿¡æ¯"))
            print(f"Worker {worker_id}: æˆåŠŸ")
        except Exception as e:
            results.append((worker_id, False, f"Worker {worker_id}: å¤±è´¥ - {e}"))
            print(f"Worker {worker_id}: å¤±è´¥ - {e}")
    
    # åˆ›å»ºå¤šä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®æ•°æ®åº“
    threads = []
    for i in range(10):
        t = threading.Thread(target=worker, args=(i,))
        threads.append(t)
        t.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        t.join()
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, success, _ in results if success)
    print(f"\nå¹¶å‘æµ‹è¯•ç»“æœ: {success_count}/10 æˆåŠŸ")
    
    return success_count > 0


def test_url_operations():
    """æµ‹è¯•URLæ“ä½œ"""
    print("\næµ‹è¯•URLæ“ä½œ...")
    
    try:
        db = UrlDatabase()
        
        # æ·»åŠ æµ‹è¯•URL
        test_url = "https://example.com/test_db_connection"
        added = db.add_url(test_url, source_url="https://example.com", depth=1)
        print(f"æ·»åŠ URL: {added}")
        
        # æ£€æŸ¥URLçŠ¶æ€
        is_crawled = db.is_crawled(test_url)
        print(f"URLæ˜¯å¦å·²æŠ“å–: {is_crawled}")
        
        # æ ‡è®°ä¸ºæ­£åœ¨æŠ“å–
        db.mark_crawling(test_url)
        print("æ ‡è®°ä¸ºæ­£åœ¨æŠ“å–")
        
        # æ ‡è®°ä¸ºæˆåŠŸ
        db.mark_success(test_url, title="æµ‹è¯•é¡µé¢", html_file_path="/test/path.html")
        print("æ ‡è®°ä¸ºæˆåŠŸ")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = db.get_stats()
        print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
        
        return True
    except Exception as e:
        print(f"URLæ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("æ•°æ®åº“è¿æ¥æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•å•ä¸ªè¿æ¥
    single_success = test_single_connection()
    
    # æµ‹è¯•å¹¶å‘è¿æ¥
    concurrent_success = test_concurrent_connections()
    
    # æµ‹è¯•URLæ“ä½œ
    url_op_success = test_url_operations()
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"  å•è¿æ¥æµ‹è¯•: {'é€šè¿‡' if single_success else 'å¤±è´¥'}")
    print(f"  å¹¶å‘æµ‹è¯•: {'é€šè¿‡' if concurrent_success else 'å¤±è´¥'}")
    print(f"  URLæ“ä½œæµ‹è¯•: {'é€šè¿‡' if url_op_success else 'å¤±è´¥'}")
    
    if single_success and concurrent_success and url_op_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ•°æ®åº“è¿æ¥æ­£å¸¸!")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œè¿æ¥ã€‚")
        return 1


if __name__ == '__main__':
    sys.exit(main())