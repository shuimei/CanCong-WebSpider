#!/usr/bin/env python3
"""
çˆ¬è™«åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import os
import sys
from webspider.database import UrlDatabase, normalize_url, is_valid_url


def test_database():
    """æµ‹è¯•æ•°æ®åº“åŠŸèƒ½"""
    print("æµ‹è¯•æ•°æ®åº“åŠŸèƒ½...")
    
    try:
        # ä½¿ç”¨PostgreSQLæ•°æ®åº“ï¼ˆä¸å†éœ€è¦ä¸´æ—¶æ•°æ®åº“ï¼‰
        db = UrlDatabase()
        
        # æµ‹è¯•æ·»åŠ URL
        test_base_url = 'https://test-example-spider.com'
        result1 = db.add_url(f'{test_base_url}/test1', depth=0)
        result2 = db.add_url(f'{test_base_url}/test2', source_url=test_base_url, depth=1)
        result3 = db.add_url(f'{test_base_url}/test1', depth=0)  # é‡å¤URL
        
        print(f"æ·»åŠ ç¬¬ä¸€ä¸ªURLç»“æœ: {result1}")
        print(f"æ·»åŠ ç¬¬äºŒä¸ªURLç»“æœ: {result2}")
        print(f"æ·»åŠ é‡å¤URLç»“æœ: {result3}")
        
        # æµ‹è¯•çŠ¶æ€æ£€æŸ¥
        is_crawled_before = db.is_crawled(f'{test_base_url}/test1')
        print(f"æµ‹è¯•URLåˆå§‹çŠ¶æ€ï¼ˆåº”ä¸ºFalseï¼‰: {is_crawled_before}")
        
        # æµ‹è¯•çŠ¶æ€æ›´æ–°
        db.mark_crawling(f'{test_base_url}/test1')
        is_crawled_after = db.is_crawled(f'{test_base_url}/test1')
        print(f"æ ‡è®°æ­£åœ¨æŠ“å–åçŠ¶æ€ï¼ˆåº”ä¸ºTrueï¼‰: {is_crawled_after}")
        
        db.mark_success(f'{test_base_url}/test1', title='Test Page', html_file_path='/test/path.html')
        
        # æµ‹è¯•ç»Ÿè®¡
        stats = db.get_stats()
        print(f"æ•°æ®åº“ç»Ÿè®¡: {stats}")
        
        print("âœ… æ•°æ®åº“åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_url_utils():
    """æµ‹è¯•URLå·¥å…·å‡½æ•°"""
    print("æµ‹è¯•URLå·¥å…·å‡½æ•°...")
    
    # æµ‹è¯•URLæ ‡å‡†åŒ–
    assert normalize_url('/path', 'https://example.com') == 'https://example.com/path'
    assert normalize_url('https://example.com/page#fragment') == 'https://example.com/page'
    
    # æµ‹è¯•URLæœ‰æ•ˆæ€§æ£€æŸ¥
    assert is_valid_url('https://example.com') == True
    assert is_valid_url('http://example.com') == True
    assert is_valid_url('ftp://example.com') == False
    assert is_valid_url('invalid-url') == False
    
    print("âœ… URLå·¥å…·å‡½æ•°æµ‹è¯•é€šè¿‡")


def test_project_structure():
    """æµ‹è¯•é¡¹ç›®ç»“æ„"""
    print("æµ‹è¯•é¡¹ç›®ç»“æ„...")
    
    required_files = [
        'scrapy.cfg',
        'webspider/__init__.py',
        'webspider/settings.py',
        'webspider/items.py',
        'webspider/database.py',
        'webspider/middlewares.py',
        'webspider/pipelines.py',
        'webspider/spiders/__init__.py',
        'webspider/spiders/webspider.py',
        'run_spider.py',
        'spider.py',
        'requirements.txt',
        'README.md'
    ]
    
    missing_files = []
    for file_path in required_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print(f"âŒ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
        return False
    
    print("âœ… é¡¹ç›®ç»“æ„å®Œæ•´")
    return True


def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    try:
        from webspider.database import UrlDatabase
        from webspider.items import UrlItem, PageItem
        from webspider.spiders.webspider import WebSpider
        print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True


def test_dependencies():
    """æµ‹è¯•ä¾èµ–åŒ…"""
    print("æµ‹è¯•Pythonä¾èµ–åŒ…...")
    
    required_packages = [
        'scrapy',
        'selenium', 
        'bs4',  # beautifulsoup4
        'lxml'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå¤±ä¾èµ–åŒ…: {missing_packages}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ•·ï¸ ç½‘é¡µçˆ¬è™«é¡¹ç›®æµ‹è¯•")
    print("=" * 40)
    
    tests = [
        test_project_structure,
        test_imports,
        test_dependencies,
        test_url_utils,
        test_database
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        print(f"æ­£åœ¨è¿è¡Œ: {test.__name__}")
        try:
            result = test()
            if result is None or result:
                passed += 1
                print(f"âœ… {test.__name__} æµ‹è¯•é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test.__name__} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test.__name__} æµ‹è¯•å¼‚å¸¸: {e}")
            failed += 1
        print()
    
    print("=" * 40)
    print(f"æµ‹è¯•å®Œæˆ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«é¡¹ç›®å°±ç»ªã€‚")
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python spider.py https://example.com")
        print("  python spider.py  # äº¤äº’å¼æ¨¡å¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
    
    return failed == 0


if __name__ == '__main__':
    sys.exit(0 if main() else 1)